{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1ba54da-e72c-4a28-a79c-031ed22d79f0",
   "metadata": {},
   "source": [
    "# üè¢ Enterprise Sales Prediction Pipeline (Production Grade)\n",
    "1.  **DevOps Automation:** CI/CD pipeline integration via GitHub Actions.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c06f876-f201-4090-843a-4bce88108d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üöÄ INITIALIZING PRODUCTION PIPELINE ---\n",
      "[SYSTEM] Environment: Linux-x86_64 | Memory: Allocated 64GB | GPU: CUDA Enabled\n",
      "\n",
      "[STAGE 1/4] Establishing secure connection to Data Warehouse...\n",
      "   > Ingesting Shard #1123: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100% Buffered\n",
      "   ‚úÖ Ingestion Complete. (15.4M Records cached)\n",
      "\n",
      "[STAGE 2/4] Running ETL Sanitization Jobs...\n",
      "   > Cluster Node 50/50: Optimizing Index & Removing Nulls...\n",
      "   ‚úÖ ETL Verification Passed.\n",
      "\n",
      "[STAGE 3/4] Training LSTM Neural Network (Time-Series)...\n",
      "   > Epoch 1/10 initiated...\n",
      "      Batch 10/10 - Loss: 0.7324 - Val_Acc: 0.6350\n",
      "   > Epoch 2/10 initiated...\n",
      "      Batch 8/10 - Loss: 0.6750 - Val_Acc: 0.6700"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# --- CORE PROCESSING MODULE ---\n",
    "print(\"--- üöÄ INITIALIZING PRODUCTION PIPELINE ---\")\n",
    "print(f\"[SYSTEM] Environment: Linux-x86_64 | Memory: Allocated 64GB | GPU: CUDA Enabled\")\n",
    "\n",
    "# --- STAGE 1: DATA INGESTION ---\n",
    "# (Simulates fetching large datasets with a delay)\n",
    "print(\"\\n[STAGE 1/4] Establishing secure connection to Data Warehouse...\")\n",
    "total_batches = 100\n",
    "for i in range(total_batches):\n",
    "    time.sleep(3) # Delay per batch\n",
    "    \n",
    "    # Professional Progress Bar Visualization\n",
    "    percent = int((i + 1) / total_batches * 100)\n",
    "    bar = '‚ñà' * (percent // 5) + '‚ñë' * (20 - (percent // 5))\n",
    "    sys.stdout.write(f\"\\r   > Ingesting Shard #{i+1024}: |{bar}| {percent}% Buffered\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "print(\"\\n   ‚úÖ Ingestion Complete. (15.4M Records cached)\")\n",
    "\n",
    "# --- STAGE 2: ETL & NORMALIZATION ---\n",
    "# (Simulates data cleaning processes)\n",
    "print(\"\\n[STAGE 2/4] Running ETL Sanitization Jobs...\")\n",
    "nodes = 50\n",
    "for i in range(nodes):\n",
    "    time.sleep(6) # Delay per node\n",
    "    \n",
    "    sys.stdout.write(f\"\\r   > Cluster Node {i+1}/{nodes}: Optimizing Index & Removing Nulls...\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "print(\"\\n   ‚úÖ ETL Verification Passed.\")\n",
    "\n",
    "# --- STAGE 3: MODEL TRAINING (Deep Learning) ---\n",
    "# (Simulates heavy AI model training)\n",
    "print(\"\\n[STAGE 3/4] Training LSTM Neural Network (Time-Series)...\")\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"   > Epoch {epoch}/{epochs} initiated...\")\n",
    "    \n",
    "    # Simulate Steps per Epoch\n",
    "    for step in range(10):\n",
    "        time.sleep(6) # Delay per step\n",
    "        # Simulated formula to make loss fluctuation look natural\n",
    "        loss = 0.8 - (0.07 * epoch) + (random.random() * 0.02)\n",
    "        acc = 0.6 + (0.035 * epoch)\n",
    "        sys.stdout.write(f\"\\r      Batch {step+1}/10 - Loss: {loss:.4f} - Val_Acc: {acc:.4f}\")\n",
    "        sys.stdout.flush()\n",
    "    print(\"\") \n",
    "\n",
    "print(\"   ‚úÖ Model Converged Successfully.\")\n",
    "\n",
    "# --- STAGE 4: POST-PROCESSING ---\n",
    "print(\"\\n[STAGE 4/4] Generating Compliance Report...\")\n",
    "checks = [\"Bias Detection\", \"Variance Analysis\", \"Security Audit\", \"API Latency Check\"]\n",
    "for check in checks:\n",
    "    sys.stdout.write(f\"\\r   > Running {check}...\")\n",
    "    sys.stdout.flush()\n",
    "    time.sleep(30) # Validation delay\n",
    "\n",
    "# Final Result\n",
    "data = {\n",
    "    'Month': ['Jan', 'Feb', 'Mar', 'Apr', 'May'],\n",
    "    'Revenue (M)': [random.randint(100, 500) for _ in range(5)]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df['AI_Forecast'] = df['Revenue (M)'] * 1.12 \n",
    "\n",
    "print(\"\\n\\n[SUCCESS] Pipeline Completed.\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e07b8fe-0462-47eb-b3f7-1a86a0a5dc27",
   "metadata": {},
   "source": [
    "# üîí Security Protocols\n",
    "\n",
    "Ensure the following secrets are configured in the repository settings!!!:\n",
    "* `TARGET_EMAIL`: For automated status reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6017143-a7aa-4660-9707-af0d438dbcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# --- DEVOPS: GENERATE WORKFLOW CONFIGURATION ---\n",
    "def create_github_workflow():\n",
    "    workflow_dir = \".github/workflows\"\n",
    "    if not os.path.exists(workflow_dir):\n",
    "        os.makedirs(workflow_dir)\n",
    "\n",
    "    # Professional Timeout and Job Name configuration\n",
    "    yaml_content = \"\"\"name: Enterprise Production Pipeline\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ \"main\" ]\n",
    "  workflow_dispatch:\n",
    "\n",
    "jobs:\n",
    "  run-production-job:\n",
    "    runs-on: ubuntu-latest\n",
    "    timeout-minutes: 60 \n",
    "    \n",
    "    steps:\n",
    "      - name: Checkout Repository\n",
    "        uses: actions/checkout@v3\n",
    "\n",
    "      - name: Initialize Environment (Python 3.9)\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.9'\n",
    "\n",
    "      - name: Install Enterprise Dependencies\n",
    "        run: |\n",
    "          sudo apt-get update && sudo apt-get install -y sendmail\n",
    "          pip install papermill jupyter ipykernel pandas\n",
    "\n",
    "      - name: Security Handshake (Secret Check)\n",
    "        run: |\n",
    "          if [ -z \"${{ secrets.TARGET_EMAIL }}\" ]; then\n",
    "            echo \"‚ùå CRITICAL: Notification Endpoint (Secret) not found.\"\n",
    "            exit 1\n",
    "          fi\n",
    "          echo \"‚úÖ Security Handshake Verified.\"\n",
    "\n",
    "      - name: Execute Main Pipeline\n",
    "        run: |\n",
    "          papermill SalesForecastPipeline.ipynb Execution_Log.ipynb\n",
    "\n",
    "      - name: Notification Service (Success)\n",
    "        if: success()\n",
    "        run: |\n",
    "          echo \"Subject: [REPORT] Enterprise Pipeline Completed Successfully\" | sendmail -v ${{ secrets.TARGET_EMAIL }}\n",
    "\n",
    "      - name: Notification Service (Failure)\n",
    "        if: failure()\n",
    "        run: |\n",
    "          echo \"Subject: [ALERT] Pipeline Execution Failed\" | sendmail -v ${{ secrets.TARGET_EMAIL }}\n",
    "\"\"\"\n",
    "\n",
    "    file_path = os.path.join(workflow_dir, \"automation_task.yml\")\n",
    "    \n",
    "    # --- FIX IS HERE: Added encoding=\"utf-8\" to support Emojis on Windows ---\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(yaml_content)\n",
    "    \n",
    "    print(f\"[DEVOPS] Workflow configuration generated at: {file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_github_workflow()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670fa71d-f523-4705-a386-f88ba8b19f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- AUTHENTICATION SETUP (SECURE INPUT) ---\n",
    "import getpass\n",
    "\n",
    "print(\"--- Authenticate Git User ---\")\n",
    "try:\n",
    "    git_email = getpass.getpass(\"User Email: \").strip()\n",
    "    git_name = getpass.getpass(\"User Name : \").strip()\n",
    "except:\n",
    "    git_email = None\n",
    "\n",
    "if git_email and git_name:\n",
    "    !git config user.email \"{git_email}\"\n",
    "    !git config user.name \"{git_name}\"\n",
    "    \n",
    "    !git init\n",
    "    !git add .\n",
    "    !git add .github -f\n",
    "    !git commit -m \"Deploy: Production Release v1.0\"\n",
    "    \n",
    "    print(f\"\\n[AUTH] User identity confirmed.\")\n",
    "else:\n",
    "    print(\"\\n[WARN] Authentication skipped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7252e4ad-a911-4a29-a891-2ecc2704a0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- REMOTE REPOSITORY LINKING ---\n",
    "import getpass\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"--- Establish Remote Connection ---\")\n",
    "print(f\"Working Directory: {os.getcwd()}\")\n",
    "\n",
    "try:\n",
    "    repo_url = getpass.getpass(\"Repository URL: \").strip()\n",
    "except:\n",
    "    repo_url = None\n",
    "\n",
    "if repo_url:\n",
    "    subprocess.run([\"git\", \"remote\", \"remove\", \"origin\"], stderr=subprocess.DEVNULL)\n",
    "    res = subprocess.run([\"git\", \"remote\", \"add\", \"origin\", repo_url], capture_output=True, text=True)\n",
    "    \n",
    "    if res.returncode == 0:\n",
    "        print(\"\\n‚úÖ [NET] Remote origin configured successfully.\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå [NET] Connection refused. Check URL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739409ba-138e-489d-9a9a-b498bd3ac94b",
   "metadata": {},
   "source": [
    "# üìÑ CI/CD Workflow Documentation\n",
    "\n",
    "This workflow is configured to automate the machine learning pipeline on **GitHub Actions**, fulfilling the three core requirements of the task.\n",
    "\n",
    "### 1. Workflow Configuration\n",
    "**Requirement:** *\"Configure a 'GitHub Actions' workflow...\"*\n",
    "\n",
    "* **Trigger:** The workflow is configured to start automatically whenever code is pushed to the `main` branch (`on: push`).\n",
    "* **Environment:** It provisions an **Ubuntu** Linux server and sets up **Python 3.9**.\n",
    "* **Dependencies:** It automatically installs the necessary libraries (`papermill`, `pandas`, `jupyter`) and the email utility (`sendmail`) required for the operation.\n",
    "\n",
    "### 2. Execution Logic\n",
    "**Requirement:** *\"...to execute 'SalesForecastPipeline.ipynb'...\"*\n",
    "\n",
    "* **Papermill Integration:** The workflow uses **Papermill** to programmatically execute the Jupyter Notebook (`SalesForecastPipeline.ipynb`).\n",
    "* **Process:** This step runs the entire data engineering and deep learning simulation (including the 20+ minute processing time) to generate the forecast report.\n",
    "\n",
    "### 3. Error Detection & Status Updates\n",
    "**Requirement:** *\"...detect execution errors, and send a status update email using 'sendmail'.\"*\n",
    "\n",
    "* **Error Detection:** The workflow utilizes GitHub Actions' **conditional logic**:\n",
    "* `if: success()`: Detects if the notebook finished correctly.\n",
    "* `if: failure()`: Automatically detects if the notebook crashed or encountered an error.\n",
    "\n",
    "* **Email Notification:** Based on the detection result, it uses the **`sendmail`** command to send a specific status update (either \"Completed Successfully\" or \"Execution Failed\") to the registered email address (`TARGET_EMAIL`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
