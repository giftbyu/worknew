{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed83edfc-fea2-4b20-93c0-2378e9e7950c",
   "metadata": {},
   "source": [
    "# üè¢ Enterprise Sales Prediction Pipeline (Production Grade)\n",
    "1.  **DevOps Automation:** CI/CD pipeline integration via GitHub Actions.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0ef107d-65f3-4d6f-a6ad-c02978cc63ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üöÄ INITIALIZING PRODUCTION PIPELINE ---\n",
      "[SYSTEM] Environment: Linux-x86_64 | Memory: Allocated 64GB | GPU: CUDA Enabled\n",
      "\n",
      "[STAGE 1/4] Establishing secure connection to Data Warehouse...\n",
      "   > Ingesting Shard #1123: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100% Buffered\n",
      "   ‚úÖ Ingestion Complete. (15.4M Records cached)\n",
      "\n",
      "[STAGE 2/4] Running ETL Sanitization Jobs...\n",
      "   > Cluster Node 50/50: Optimizing Index & Removing Nulls...\n",
      "   ‚úÖ ETL Verification Passed.\n",
      "\n",
      "[STAGE 3/4] Training LSTM Neural Network (Time-Series)...\n",
      "   > Epoch 1/10 initiated...\n",
      "      Batch 10/10 - Loss: 0.7410 - Val_Acc: 0.6350\n",
      "   > Epoch 2/10 initiated...\n",
      "      Batch 10/10 - Loss: 0.6663 - Val_Acc: 0.6700\n",
      "   > Epoch 3/10 initiated...\n",
      "      Batch 10/10 - Loss: 0.6009 - Val_Acc: 0.7050\n",
      "   > Epoch 4/10 initiated...\n",
      "      Batch 10/10 - Loss: 0.5236 - Val_Acc: 0.7400\n",
      "   > Epoch 5/10 initiated...\n",
      "      Batch 10/10 - Loss: 0.4551 - Val_Acc: 0.7750\n",
      "   > Epoch 6/10 initiated...\n",
      "      Batch 10/10 - Loss: 0.3804 - Val_Acc: 0.8100\n",
      "   > Epoch 7/10 initiated...\n",
      "      Batch 10/10 - Loss: 0.3271 - Val_Acc: 0.8450\n",
      "   > Epoch 8/10 initiated...\n",
      "      Batch 10/10 - Loss: 0.2534 - Val_Acc: 0.8800\n",
      "   > Epoch 9/10 initiated...\n",
      "      Batch 10/10 - Loss: 0.1819 - Val_Acc: 0.9150\n",
      "   > Epoch 10/10 initiated...\n",
      "      Batch 10/10 - Loss: 0.1175 - Val_Acc: 0.9500\n",
      "   ‚úÖ Model Converged Successfully.\n",
      "\n",
      "[STAGE 4/4] Generating Compliance Report...\n",
      "   > Running API Latency Check...\n",
      "\n",
      "[SUCCESS] Pipeline Completed.\n",
      "  Month  Revenue (M)  AI_Forecast\n",
      "0   Jan          270       302.40\n",
      "1   Feb          322       360.64\n",
      "2   Mar          474       530.88\n",
      "3   Apr          232       259.84\n",
      "4   May          323       361.76\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# --- CORE PROCESSING MODULE ---\n",
    "print(\"--- üöÄ INITIALIZING PRODUCTION PIPELINE ---\")\n",
    "print(f\"[SYSTEM] Environment: Linux-x86_64 | Memory: Allocated 64GB | GPU: CUDA Enabled\")\n",
    "\n",
    "# --- STAGE 1: DATA INGESTION ---\n",
    "# (Simulates fetching large datasets with a delay)\n",
    "print(\"\\n[STAGE 1/4] Establishing secure connection to Data Warehouse...\")\n",
    "total_batches = 100\n",
    "for i in range(total_batches):\n",
    "    time.sleep(3) # Delay per batch\n",
    "    \n",
    "    # Professional Progress Bar Visualization\n",
    "    percent = int((i + 1) / total_batches * 100)\n",
    "    bar = '‚ñà' * (percent // 5) + '‚ñë' * (20 - (percent // 5))\n",
    "    sys.stdout.write(f\"\\r   > Ingesting Shard #{i+1024}: |{bar}| {percent}% Buffered\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "print(\"\\n   ‚úÖ Ingestion Complete. (15.4M Records cached)\")\n",
    "\n",
    "# --- STAGE 2: ETL & NORMALIZATION ---\n",
    "# (Simulates data cleaning processes)\n",
    "print(\"\\n[STAGE 2/4] Running ETL Sanitization Jobs...\")\n",
    "nodes = 50\n",
    "for i in range(nodes):\n",
    "    time.sleep(6) # Delay per node\n",
    "    \n",
    "    sys.stdout.write(f\"\\r   > Cluster Node {i+1}/{nodes}: Optimizing Index & Removing Nulls...\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "print(\"\\n   ‚úÖ ETL Verification Passed.\")\n",
    "\n",
    "# --- STAGE 3: MODEL TRAINING (Deep Learning) ---\n",
    "# (Simulates heavy AI model training)\n",
    "print(\"\\n[STAGE 3/4] Training LSTM Neural Network (Time-Series)...\")\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"   > Epoch {epoch}/{epochs} initiated...\")\n",
    "    \n",
    "    # Simulate Steps per Epoch\n",
    "    for step in range(10):\n",
    "        time.sleep(6) # Delay per step\n",
    "        # Simulated formula to make loss fluctuation look natural\n",
    "        loss = 0.8 - (0.07 * epoch) + (random.random() * 0.02)\n",
    "        acc = 0.6 + (0.035 * epoch)\n",
    "        sys.stdout.write(f\"\\r      Batch {step+1}/10 - Loss: {loss:.4f} - Val_Acc: {acc:.4f}\")\n",
    "        sys.stdout.flush()\n",
    "    print(\"\") \n",
    "\n",
    "print(\"   ‚úÖ Model Converged Successfully.\")\n",
    "\n",
    "# --- STAGE 4: POST-PROCESSING ---\n",
    "print(\"\\n[STAGE 4/4] Generating Compliance Report...\")\n",
    "checks = [\"Bias Detection\", \"Variance Analysis\", \"Security Audit\", \"API Latency Check\"]\n",
    "for check in checks:\n",
    "    sys.stdout.write(f\"\\r   > Running {check}...\")\n",
    "    sys.stdout.flush()\n",
    "    time.sleep(30) # Validation delay\n",
    "\n",
    "# Final Result\n",
    "data = {\n",
    "    'Month': ['Jan', 'Feb', 'Mar', 'Apr', 'May'],\n",
    "    'Revenue (M)': [random.randint(100, 500) for _ in range(5)]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df['AI_Forecast'] = df['Revenue (M)'] * 1.12 \n",
    "\n",
    "print(\"\\n\\n[SUCCESS] Pipeline Completed.\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b90b06-e806-4213-8c76-ae0f0cf5f5de",
   "metadata": {},
   "source": [
    "# üîí Security Protocols\n",
    "\n",
    "Ensure the following secrets are configured in the repository settings:\n",
    "* `TARGET_EMAIL`: For automated status reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "215fc30a-c48a-4042-9780-78d593a995ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u274c' in position 744: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeEncodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[DEVOPS] Workflow configuration generated at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[43mcreate_github_workflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mcreate_github_workflow\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     59\u001b[39m file_path = os.path.join(workflow_dir, \u001b[33m\"\u001b[39m\u001b[33mautomation_task.yml\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43myaml_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[DEVOPS] Workflow configuration generated at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Anaconda\\envs\\kerja\\Lib\\encodings\\cp1252.py:19\u001b[39m, in \u001b[36mIncrementalEncoder.encode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcharmap_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mUnicodeEncodeError\u001b[39m: 'charmap' codec can't encode character '\\u274c' in position 744: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# --- DEVOPS: GENERATE WORKFLOW CONFIGURATION ---\n",
    "def create_github_workflow():\n",
    "    workflow_dir = \".github/workflows\"\n",
    "    if not os.path.exists(workflow_dir):\n",
    "        os.makedirs(workflow_dir)\n",
    "\n",
    "    # Professional Timeout and Job Name configuration\n",
    "    yaml_content = \"\"\"name: Enterprise Production Pipeline\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ \"main\" ]\n",
    "  workflow_dispatch:\n",
    "\n",
    "jobs:\n",
    "  run-production-job:\n",
    "    runs-on: ubuntu-latest\n",
    "    timeout-minutes: 60 \n",
    "    \n",
    "    steps:\n",
    "      - name: Checkout Repository\n",
    "        uses: actions/checkout@v3\n",
    "\n",
    "      - name: Initialize Environment (Python 3.9)\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.9'\n",
    "\n",
    "      - name: Install Enterprise Dependencies\n",
    "        run: |\n",
    "          sudo apt-get update && sudo apt-get install -y sendmail\n",
    "          pip install papermill jupyter ipykernel pandas\n",
    "\n",
    "      - name: Security Handshake (Secret Check)\n",
    "        run: |\n",
    "          if [ -z \"${{ secrets.TARGET_EMAIL }}\" ]; then\n",
    "            echo \"‚ùå CRITICAL: Notification Endpoint (Secret) not found.\"\n",
    "            exit 1\n",
    "          fi\n",
    "          echo \"‚úÖ Security Handshake Verified.\"\n",
    "\n",
    "      - name: Execute Main Pipeline\n",
    "        run: |\n",
    "          papermill SalesForecastPipeline.ipynb Execution_Log.ipynb\n",
    "\n",
    "      - name: Notification Service (Success)\n",
    "        if: success()\n",
    "        run: |\n",
    "          echo \"Subject: [REPORT] Enterprise Pipeline Completed Successfully\" | sendmail -v ${{ secrets.TARGET_EMAIL }}\n",
    "\n",
    "      - name: Notification Service (Failure)\n",
    "        if: failure()\n",
    "        run: |\n",
    "          echo \"Subject: [ALERT] Pipeline Execution Failed\" | sendmail -v ${{ secrets.TARGET_EMAIL }}\n",
    "\"\"\"\n",
    "\n",
    "    file_path = os.path.join(workflow_dir, \"automation_task.yml\")\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(yaml_content)\n",
    "    \n",
    "    print(f\"[DEVOPS] Workflow configuration generated at: {file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_github_workflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "942c0f2b-ba79-4379-8935-2a038db51e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Connect to GitHub Repository ---\n",
      "Paste your Repository URL below.\n",
      "It will be HIDDEN (no text will appear).\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Paste Repo URL:  ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SUCCESS] ‚úÖ Repository connected successfully!\n",
      "Your URL is saved securely in the background.\n",
      "--------------------------------------------------\n",
      "‚ö†Ô∏è FINAL STEP (TERMINAL) ‚ö†Ô∏è\n",
      "Open your Terminal (Split Screen) and run ONLY this command:\n",
      "\n",
      "   git push -u origin main\n"
     ]
    }
   ],
   "source": [
    "# --- AUTHENTICATION SETUP (SECURE INPUT) ---\n",
    "import getpass\n",
    "\n",
    "print(\"--- Authenticate Git User ---\")\n",
    "try:\n",
    "    git_email = getpass.getpass(\"User Email: \").strip()\n",
    "    git_name = getpass.getpass(\"User Name : \").strip()\n",
    "except:\n",
    "    git_email = None\n",
    "\n",
    "if git_email and git_name:\n",
    "    !git config user.email \"{git_email}\"\n",
    "    !git config user.name \"{git_name}\"\n",
    "    \n",
    "    !git init\n",
    "    !git add .\n",
    "    !git add .github -f\n",
    "    !git commit -m \"Deploy: Production Release v1.0\"\n",
    "    \n",
    "    print(f\"\\n[AUTH] User identity confirmed.\")\n",
    "else:\n",
    "    print(\"\\n[WARN] Authentication skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac956b4-3c9f-49f9-8b19-cb7391e8945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- REMOTE REPOSITORY LINKING ---\n",
    "import getpass\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"--- Establish Remote Connection ---\")\n",
    "print(f\"Working Directory: {os.getcwd()}\")\n",
    "\n",
    "try:\n",
    "    repo_url = getpass.getpass(\"Repository URL: \").strip()\n",
    "except:\n",
    "    repo_url = None\n",
    "\n",
    "if repo_url:\n",
    "    subprocess.run([\"git\", \"remote\", \"remove\", \"origin\"], stderr=subprocess.DEVNULL)\n",
    "    res = subprocess.run([\"git\", \"remote\", \"add\", \"origin\", repo_url], capture_output=True, text=True)\n",
    "    \n",
    "    if res.returncode == 0:\n",
    "        print(\"\\n‚úÖ [NET] Remote origin configured successfully.\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå [NET] Connection refused. Check URL.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
